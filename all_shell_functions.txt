function default_settings {
export PACKAGES="com.databricks:spark-csv_2.11:1.3.0"
export SCHEDULER="spark.scheduler.mode=FIFO"
export EXTRAJAVAOPTIONS="spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps"
export JARS="/home/hduser/jars/spark-streaming-kafka-assembly_2.11-1.6.1.jar"
export SPARKUI="spark.ui.port="
export SPARKDRIVERPORT="spark.driver.port=54631"
export SPARKFILESERVERPORT="spark.fileserver.port=54731"
export SPARKBLOCKMANAGERPORT="spark.blockManager.port=54832"
export SPARKKRYOSERIALIZERBUFFERMAX="spark.kryoserializer.buffer.max=512"
}
#
function run_yarn {
default_settings
if [[ -z ${SP} ]]
then
        SPARKUIPORT="${SPARKU}55555"
else
        SPARKUIPORT="${SPARKUI}${SP}"
fi

${SPARK_HOME}/bin/spark-submit \
                --packages ${PACKAGES} \
                --executor-memory 4G \
                --num-executors 1 \
                --executor-cores 2 \
                --master yarn \
                --deploy-mode client \
                --conf "${SCHEDULER}" \
                --conf "${EXTRAJAVAOPTIONS}" \
                --jars ${JARS} \
                --class "${FILE_NAME}" \
                --conf "${SPARKUIPORT}" \
                --conf "${SPARKDRIVERPORT}" \
                --conf "${SPARKFILESERVERPORT}" \
                --conf "${SPARKBLOCKMANAGERPORT}" \
                --conf "${SPARKKRYOSERIALIZERBUFFERMAX}" \
                --conf spark.executor.memoryOverhead=1000 \
                ${JAR_FILE}
}
#
function create_assembly_sbt_file {
        ASSEMBLY_SBT_FILE=${GEN_APPSDIR}/scala/${APPLICATION}/project/assembly.sbt
        [ -f ${ASSEMBLY_SBT_FILE} ] && rm -f ${ASSEMBLY_SBT_FILE}
        cat >> $ASSEMBLY_SBT_FILE << !
addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.6")
//addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.12.0")
!
}
#
function create_build_sbt_file {
        BUILD_SBT_FILE=${GEN_APPSDIR}/scala/${APPLICATION}/build.sbt
        [ -f ${BUILD_SBT_FILE} ] && rm -f ${BUILD_SBT_FILE}
        cat >> $BUILD_SBT_FILE << !
lazy val root = (project in file(".")).
  settings(
    name := "${APPLICATION}",
    version := "1.0",
    scalaVersion := "2.11.8",
    mainClass in Compile := Some("myPackage.${APPLICATION}")
  )
assemblyShadeRules in assembly := Seq(
ShadeRule.rename("com.google.common.**" -> "my_conf.@1").inAll
)

libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.0.0" % "provided"
libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "2.4.0"
libraryDependencies += "org.apache.spark" %% "spark-core" % "2.0.0"  % "provided" exclude("org.apache.hadoop", "hadoop-client")
resolvers += "Akka Repository" at "http://repo.akka.io/releases/"
libraryDependencies += "com.amazonaws" % "aws-java-sdk" % "1.7.8"
libraryDependencies += "commons-io" % "commons-io" % "2.4"
libraryDependencies += "javax.servlet" % "javax.servlet-api" % "3.0.1" % "provided"
libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.0.0" % "provided"
libraryDependencies += "org.apache.spark" %% "spark-hive" % "2.0.0" % "provided"
libraryDependencies += "com.google.cloud.bigdataoss" % "bigquery-connector" % "0.13.4-hadoop3"
libraryDependencies += "com.google.cloud.bigdataoss" % "gcs-connector" % "1.9.4-hadoop3"
libraryDependencies += "com.google.code.gson" % "gson" % "2.8.5"
libraryDependencies += "org.apache.httpcomponents" % "httpcore" % "4.4.8"
libraryDependencies += "org.apache.hadoop" % "hadoop-hdfs" % "2.4.0"
libraryDependencies += "com.github.samelamin" %% "spark-bigquery" % "0.2.5"
libraryDependencies += "org.apache.spark" %% "spark-mllib" % "2.2.0"

// META-INF discarding

assemblyMergeStrategy in assembly := {
 case PathList("META-INF", "MANIFEST.MF") => MergeStrategy.discard
 case PathList("META-INF", xs @ _*) => MergeStrategy.discard
 case x => MergeStrategy.first
}
!
}
#


